{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Flight Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This initial section explores the data and clean it for later modeling. This section also to completes the requirements for the Capstone Milestone report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation, ensemble, tree, metrics, preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, date\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "le = preprocessing.LabelEncoder()\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training files are divided into three separate files in six month increments\n",
    "temp_train_2014_1 = pd.read_csv('CAX_Train_2014_Jan_to_Jun.csv')\n",
    "temp_train_2014_2 = pd.read_csv('CAX_Train_2014_Jul_to_Dec.csv')\n",
    "temp_train_2015 = pd.read_csv('CAX_Train_2015.csv')\n",
    "\n",
    "# Read in the weather file\n",
    "weather = pd.read_csv('weather.csv')\n",
    "\n",
    "# The codes file is a manual matching I created between the IATA codes in the training files and\n",
    "# the ICAO codes in the weather file. Multiple IATA codes can join to the same ICAO codes based on proximity to \n",
    "# cities. City names in the training files can be truncated and often did not match the weather file. If the\n",
    "# matching wasn't obvious based on the IATA and ICAO codes then IATA code were looked up in Google to determine the\n",
    "# nearest ICAO city.\n",
    "codes = pd.read_csv('training_weather_cities_joined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creating week of year field in order to group weather events by week\n",
    "weather['week_of_year'] = pd.to_datetime(weather['date'], \n",
    "                                         errors='coerce').dt.weekofyear.astype(int)\n",
    "\n",
    "# Some basic munging of weather fields\n",
    "weather.events.fillna(-1, inplace=True)\n",
    "weather.conditions.fillna(-1, inplace=True)\n",
    "weather.gust_speed_mph.replace('-', -1, inplace=True)\n",
    "weather.gust_speed_mph.fillna(-1, inplace=True)\n",
    "weather.gust_speed_mph = pd.to_numeric(weather.gust_speed_mph, errors='coerce')\n",
    "weather.wind_speed_mph.replace('Calm', 0, inplace=True)\n",
    "weather.wind_speed_mph.fillna(-1, inplace=True)\n",
    "weather.wind_speed_mph = pd.to_numeric(weather.wind_speed_mph, errors='coerce')\n",
    "\n",
    "# Changing to weather types in the events field to numeric values based on the likelihood of causing delays\n",
    "# e.g. Events involving ice have a higher value\n",
    "weather.events.replace(['Fog', 'Fog-Hail-Thunderstorm', 'Fog-Rain', 'Fog-Rain-Hail-Thunderstorm', \n",
    "                        'Fog-Rain-Snow', 'Fog-Rain-Thunderstorm', 'Fog-Rain-Thunderstorm-Tornado', \n",
    "                        'Fog-Snow', 'Fog-Snow-Thunderstorm', 'Fog-Thunderstorm', 'Hail', \n",
    "                        'Hail-Thunderstorm', 'Rain', 'Rain-Hail', 'Rain-Hail-Thunderstorm', \n",
    "                        'Rain-Snow', 'Rain-Snow-Thunderstorm', 'Rain-Thunderstorm', \n",
    "                        'Rain-Thunderstorm-Tornado', 'Rain-Tornado', 'Snow', 'Snow-Hail', \n",
    "                        'Snow-Thunderstorm', 'Snow-Tornado', 'Thunderstorm', \n",
    "                        'Thunderstorm-Tornado', 'Tornado'], [10,\n",
    "80,\n",
    "20,\n",
    "80,\n",
    "60,\n",
    "50,\n",
    "100,\n",
    "60,\n",
    "60,\n",
    "50,\n",
    "80,\n",
    "80,\n",
    "20,\n",
    "80,\n",
    "80,\n",
    "60,\n",
    "60,\n",
    "50,\n",
    "100,\n",
    "100,\n",
    "60,\n",
    "80,\n",
    "60,\n",
    "100,\n",
    "50,\n",
    "100,\n",
    "100], inplace=True)\n",
    "\n",
    "# As with the events field, changing the conditions field to numeric values based on the likelihood of causing delays\n",
    "weather.conditions.replace(['Unknown',\n",
    "'Clear',\n",
    "'Overcast',\n",
    "'Partly Cloudy',\n",
    "'Drizzle',\n",
    "'Scattered Clouds',\n",
    "'Mostly Cloudy',\n",
    "'Haze',\n",
    "'Mist',\n",
    "'Patches of Fog',\n",
    "'Rain',\n",
    "'Shallow Fog',\n",
    "'Low Drifting Snow',\n",
    "'Rain Showers',\n",
    "'Light Blowing Snow',\n",
    "'Light Drizzle',\n",
    "'Light Fog',\n",
    "'Light Hail',\n",
    "'Light Ice Pellets',\n",
    "'Light Low Drifting Snow',\n",
    "'Light Mist',\n",
    "'Light Rain',\n",
    "'Light Rain Showers',\n",
    "'Light Sand',\n",
    "'Light Small Hail Showers',\n",
    "'Light Smoke',\n",
    "'Light Snow',\n",
    "'Light Snow Grains',\n",
    "'Light Snow Showers',\n",
    "'Light Thunderstorm',\n",
    "'Light Thunderstorms and Rain',\n",
    "'Light Thunderstorms and Snow',\n",
    "'Light Thunderstorms with Hail',\n",
    "'Light Thunderstorms with Small Hail',\n",
    "'Heavy Blowing Snow',\n",
    "'Heavy Drizzle',\n",
    "'Heavy Freezing Drizzle',\n",
    "'Heavy Freezing Fog',\n",
    "'Heavy Freezing Rain',\n",
    "'Heavy Ice Pellets',\n",
    "'Heavy Rain',\n",
    "'Heavy Rain Showers',\n",
    "'Heavy Small Hail',\n",
    "'Heavy Snow',\n",
    "'Heavy Snow Showers',\n",
    "'Heavy Thunderstorm',\n",
    "'Heavy Thunderstorms and Rain',\n",
    "'Heavy Thunderstorms and Snow',\n",
    "'Heavy Thunderstorms with Hail',\n",
    "'Heavy Thunderstorms with Small Hail',\n",
    "'Ice Crystals',\n",
    "'Ice Pellets',\n",
    "'Funnel Cloud',\n",
    "'Blowing Sand',\n",
    "'Blowing Snow',\n",
    "'Thunderstorm',\n",
    "'Thunderstorms and Rain',\n",
    "'Thunderstorms and Snow',\n",
    "'Thunderstorms with Hail',\n",
    "'Thunderstorms with Small Hail',\n",
    "'Volcanic Ash',\n",
    "'Widespread Dust',\n",
    "'Sand',\n",
    "'Sandstorm',\n",
    "'Small Hail',\n",
    "'Small Hail Showers',\n",
    "'Smoke',\n",
    "'Snow',\n",
    "'Snow Grains',\n",
    "'Snow Showers',\n",
    "'Squalls',\n",
    "'Fog',\n",
    "'Freezing Rain',\n",
    "'Hail',\n",
    "'Light Freezing Drizzle',\n",
    "'Light Freezing Fog',\n",
    "'Light Freezing Rain'], [-1,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "10,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "0,\n",
    "10,\n",
    "10,\n",
    "10,\n",
    "30,\n",
    "10,\n",
    "30,\n",
    "10,\n",
    "10,\n",
    "50,\n",
    "70,\n",
    "30,\n",
    "0,\n",
    "10,\n",
    "10,\n",
    "30,\n",
    "50,\n",
    "20,\n",
    "30,\n",
    "70,\n",
    "30,\n",
    "20,\n",
    "20,\n",
    "40,\n",
    "60,\n",
    "60,\n",
    "80,\n",
    "20,\n",
    "70,\n",
    "70,\n",
    "90,\n",
    "80,\n",
    "20,\n",
    "20,\n",
    "70,\n",
    "80,\n",
    "80,\n",
    "50,\n",
    "30,\n",
    "80,\n",
    "80,\n",
    "80,\n",
    "70,\n",
    "70,\n",
    "90,\n",
    "70,\n",
    "70,\n",
    "40,\n",
    "30,\n",
    "70,\n",
    "80,\n",
    "80,\n",
    "100,\n",
    "20,\n",
    "60,\n",
    "80,\n",
    "60,\n",
    "70,\n",
    "20,\n",
    "60,\n",
    "60,\n",
    "60,\n",
    "70,\n",
    "10,\n",
    "80,\n",
    "50,\n",
    "60,\n",
    "50,\n",
    "80], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grouping the weather features by week of year\n",
    "weather_group = weather.groupby( [ 'airport_code', 'week_of_year'] ).mean()\n",
    "weather_group = weather_group.reset_index()\n",
    "weather_group = weather_group.drop('zip', 1)\n",
    "weather_group.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This array defines the dates of holidays in 2014 and 2015\n",
    "holidays = [\n",
    "        date(2014, 1, 1), date(2014, 1, 20), date(2014, 5, 26), date(2014, 7, 4), \\\n",
    "        date(2014, 9, 1), date(2014, 11, 27), date(2014, 12, 25), \\\n",
    "        date(2015, 1, 1), date(2015, 1, 19), date(2015, 5, 25), date(2015, 7, 4), \\\n",
    "        date(2015, 9, 7), date(2015, 11, 26), date(2015, 12, 25) \\\n",
    "     ]\n",
    "\n",
    "#Function that gives the number of days from a date to the nearest holiday\n",
    "def days_from_nearest_holiday(year, month, day):\n",
    "  d = date(year, month, day)\n",
    "  x = [(abs(d-h)).days for h in holidays]\n",
    "  return min(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Combine all of the training files into one dataframe\n",
    "training = pd.concat([temp_train_2014_1, temp_train_2014_2, temp_train_2015], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Merge the codes file to get the IACO codes for the origin IATA code\n",
    "training = pd.merge(training, codes[['ORIGIN', 'city', 'airport_code']], left_on='ORIGIN', right_on='ORIGIN')\n",
    "training.rename(columns={'city': 'origin_city', 'airport_code': 'origin_icao_code'}, inplace=True)\n",
    "\n",
    "# Merge the codes file to get the IACO codes for the destination IATA code\n",
    "training = pd.merge(training, codes[['ORIGIN', 'city', 'airport_code']], left_on='DEST', right_on='ORIGIN')\n",
    "training.rename(columns={'city': 'dest_city', 'airport_code': 'dest_icao_code', 'ORIGIN_x': 'ORIGIN'}, inplace=True)\n",
    "training = training.drop('ORIGIN_y', 1)\n",
    "\n",
    "# Create day, week, and hour features from the flight date\n",
    "training['DAY_OF_YEAR'] = pd.to_datetime(training['FL_DATE'], errors='coerce').dt.dayofyear.astype(int)\n",
    "training['WEEK_OF_YEAR'] = pd.to_datetime(training['FL_DATE'], errors='coerce').dt.weekofyear.astype(int)\n",
    "training['hour'] = training['CRS_DEP_TIME'].map(lambda x: int(str(int(x)).zfill(4)[:2]))\n",
    "training['day'] = pd.to_datetime(training['FL_DATE'], errors='coerce').dt.day.astype(int)\n",
    "\n",
    "# Create the days from holiday feature using the function defined earlier\n",
    "training['days_from_holidays'] = [days_from_nearest_holiday(r.YEAR, r.MONTH, r.day) for i,r in training.iterrows()]\n",
    "\n",
    "# Merge the weather data with the training data on the origin and week of year\n",
    "training = pd.merge(training, weather_group, left_on=['origin_icao_code', 'WEEK_OF_YEAR'], \n",
    "                    right_on=['airport_code', 'week_of_year'], how='left')\n",
    "training.rename(columns={'conditions': 'orig_conditions', 'airport_code': 'orig_airport_code', \n",
    "                         'week_of_year': 'orig_week_of_year', 'temperature_f': 'orig_temperature_f', \n",
    "                         'dew_point_f': 'orig_dew_point_f', 'humidity': 'orig_humidity', \n",
    "                         'sea_level_pressure_in': 'orig_sea_level_pressure_in', \n",
    "                         'visibility_mph': 'orig_visibility_mph', 'wind_speed_mph': 'orig_wind_speed_mph', \n",
    "                         'gust_speed_mph': 'orig_gust_speed_mph', 'precipitation_in': 'orig_precipitation_in', \n",
    "                         'events': 'orig_events', 'wind_dir_degrees': 'orig_wind_dir_degrees', \n",
    "                         'event_names': 'orig_event_names', 'condition_names': 'orig_condition_names'}, inplace=True)\n",
    "\n",
    "# Merge the weather data with the training data on the destination and week of year\n",
    "training = pd.merge(training, weather_group, left_on=['dest_icao_code', 'WEEK_OF_YEAR'], \n",
    "                    right_on=['airport_code', 'week_of_year'], how='left')\n",
    "training.rename(columns={'conditions': 'dest_conditions', 'airport_code': 'dest_airport_code', \n",
    "                         'week_of_year': 'dest_week_of_year', 'temperature_f': 'dest_temperature_f', \n",
    "                         'dew_point_f': 'dest_dew_point_f', 'humidity': 'dest_humidity', \n",
    "                         'sea_level_pressure_in': 'dest_sea_level_pressure_in', \n",
    "                         'visibility_mph': 'dest_visibility_mph', 'wind_speed_mph': 'dest_wind_speed_mph', \n",
    "                         'gust_speed_mph': 'dest_gust_speed_mph', 'precipitation_in': 'dest_precipitation_in', \n",
    "                         'events': 'dest_events', 'wind_dir_degrees': 'dest_wind_dir_degrees', \n",
    "                         'event_names': 'dest_event_names', 'condition_names': 'dest_condition_names'}, inplace=True)\n",
    "\n",
    "training.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights per month\n",
    "grouped = training[['ARR_DEL15', 'MONTH']].groupby('MONTH').mean()\n",
    "\n",
    "# plot average delays by month\n",
    "grouped.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights per month\n",
    "grouped = training[['ARR_DEL15', 'DAY_OF_MONTH']].groupby('DAY_OF_MONTH').mean()\n",
    "\n",
    "# plot average delays by month\n",
    "grouped.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights by hour\n",
    "grouped = training[['ARR_DEL15', 'hour']].groupby('hour').mean()\n",
    "\n",
    "# plot average delays by hour of day\n",
    "grouped.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights per carrier\n",
    "grouped1 = training[['ARR_DEL15', 'UNIQUE_CARRIER']].groupby('UNIQUE_CARRIER').filter(lambda x: len(x)>15)\n",
    "grouped2 = grouped1.groupby('UNIQUE_CARRIER').mean()\n",
    "carrier = grouped2.sort(['ARR_DEL15'], ascending=False)\n",
    "\n",
    "# display top 15 destination carriers by delay\n",
    "carrier.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights per origin airport\n",
    "grouped1 = training[['ARR_DEL15', 'ORIGIN']].groupby('ORIGIN').filter(lambda x: len(x)>15)\n",
    "grouped2 = grouped1.groupby('ORIGIN').mean()\n",
    "carrier = grouped2.sort(['ARR_DEL15'], ascending=False)\n",
    "\n",
    "# display top 15 origin airports by delay\n",
    "carrier[:15].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There's a large difference in the amount of data for delayed vs. on time flights\n",
    "print len(training[training['ARR_DEL15'] == 1]), len(training[training['ARR_DEL15'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Define the features in order to export the training set to a csv for quicker reinput\n",
    "features_for_export = [c for c in training.columns if c in ['ARR_DEL15', 'days_from_holidays', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'DISTANCE', 'hour', 'orig_temperature_f', 'orig_dew_point_f', 'orig_humidity', 'orig_sea_level_pressure_in', 'orig_visibility_mph', 'orig_wind_speed_mph', 'orig_gust_speed_mph', 'orig_precipitation_in', 'orig_events', 'orig_conditions', 'orig_wind_dir_degrees', 'dest_temperature_f', 'dest_dew_point_f', 'dest_humidity', 'dest_sea_level_pressure_in', 'dest_visibility_mph', 'dest_wind_speed_mph', 'dest_gust_speed_mph', 'dest_precipitation_in', 'dest_events', 'dest_conditions', 'dest_wind_dir_degrees']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "training[features_for_export].to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reduce the number of on time training instances so that the model will fit more easily into memory\n",
    "# Keep all delayed instances and take random instances of on time flights equal to the number of\n",
    "# delayed instances\n",
    "delayed = training[training['ARR_DEL15'] == 1]\n",
    "on_time = training[training['ARR_DEL15'] == 0]\n",
    "sample_rows = np.random.choice(on_time.index.values, len(delayed))\n",
    "on_time = on_time.ix[sample_rows]\n",
    "training = pd.concat([on_time, delayed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation, ensemble, tree, metrics, preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, date\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "le = preprocessing.LabelEncoder()\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Create training set from combined data already saved to csv\n",
    "training = pd.read_csv('combined_data.csv', iterator=True, chunksize=100000)\n",
    "training = pd.concat([chunk for chunk in training], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Define the features to use\n",
    "features = [c for c in training.columns if c in ['days_from_holidays', 'DAY_OF_MONTH', 'DAY_OF_WEEK', \n",
    "                                                 'UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'DISTANCE', 'hour', \n",
    "                                                 'orig_temperature_f', 'orig_dew_point_f', 'orig_humidity', \n",
    "                                                 'orig_sea_level_pressure_in', 'orig_visibility_mph', \n",
    "                                                 'orig_wind_speed_mph', 'orig_gust_speed_mph', \n",
    "                                                 'orig_precipitation_in', 'orig_events', 'orig_conditions', \n",
    "                                                 'orig_wind_dir_degrees', 'dest_temperature_f', 'dest_dew_point_f', \n",
    "                                                 'dest_humidity', 'dest_sea_level_pressure_in', \n",
    "                                                 'dest_visibility_mph', 'dest_wind_speed_mph', \n",
    "                                                 'dest_gust_speed_mph', 'dest_precipitation_in', \n",
    "                                                 'dest_events', 'dest_conditions', 'dest_wind_dir_degrees']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "labels = training['ARR_DEL15']\n",
    "training = training.drop('ARR_DEL15', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Transform the string labels to numerical using label encoder \n",
    "training.UNIQUE_CARRIER = le.fit_transform(training.UNIQUE_CARRIER)\n",
    "training.ORIGIN = le.fit_transform(training.ORIGIN)\n",
    "training.DEST = le.fit_transform(training.DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "categ = [list(training.columns).index(x) for x in 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'hour']\n",
    "enc = preprocessing.OneHotEncoder(categorical_features = categ)\n",
    "training['UNIQUE_CARRIER'] = pd.factorize(training['UNIQUE_CARRIER'])[0]\n",
    "training['ORIGIN'] = pd.factorize(training['ORIGIN'])[0]\n",
    "training['DEST'] = pd.factorize(training['DEST'])[0]\n",
    "sparse_train = enc.fit_transform(training)\n",
    "print sparse_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create training and testing sets using cross validation\n",
    "features_train_1, features_test_1, labels_train_1, labels_test_1 = cross_validation.train_test_split(sparse_train, labels, test_size=0.70)\n",
    "features_train_2, features_test_2, labels_train_2, labels_test_2 = cross_validation.train_test_split(features_train_1, labels_train_1, test_size=0.70)\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features_train_2, labels_train_2, test_size=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#d_tree = tree.DecisionTreeClassifier(min_samples_split=1000).fit(features_train, labels_train)\n",
    "# Best so far: clf_rf = ensemble.RandomForestClassifier(n_estimators=50, n_jobs=-1, min_samples_split=50).fit(features_train, labels_train)\n",
    "# forest = ensemble.RandomForestClassifier(n_estimators=50, n_jobs=-1).fit(features_train, labels_train)\n",
    "# naive = GaussianNB().fit(features_train, labels_train)\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=100, n_jobs=-1).fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate on test set\n",
    "pred = forest.predict(features_test)\n",
    "\n",
    "\n",
    "# print results\n",
    "cm = metrics.confusion_matrix(labels_test, pred)\n",
    "print(\"Confusion matrix\")\n",
    "print(pd.DataFrame(cm))\n",
    "\n",
    "report_rf = metrics.precision_recall_fscore_support(list(labels_test), list(pred), average='micro')\n",
    "print \"precision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\\n\" % \\\n",
    "        (report_rf[0], report_rf[1], report_rf[2], metrics.roc_auc_score(list(labels_test.values), list(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def modelfit(alg, features, labels, useTrainCV=True, cv_folds=5, early_stopping_rounds=20):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(features, label=labels)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics=['auc'], early_stopping_rounds=early_stopping_rounds, show_progress=True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(features, labels, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(features)\n",
    "    dtrain_predprob = alg.predict_proba(features)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print \"\\nModel Report\"\n",
    "    print \"Accuracy : %.4g\" % metrics.accuracy_score(labels.values, dtrain_predictions)\n",
    "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(labels, dtrain_predprob)\n",
    "                    \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_final =  xgb.XGBClassifier(\n",
    " learning_rate = 1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(xgb_final, features_train_1, labels_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "boost =  xgb.XGBClassifier(\n",
    " learning_rate = 1,\n",
    " n_estimators=329,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27).fit(features_train_1, labels_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate on test set\n",
    "pred = boost.predict(features_test_1)\n",
    "\n",
    "\n",
    "# print results\n",
    "cm = metrics.confusion_matrix(labels_test_1, pred)\n",
    "print(\"Confusion matrix\")\n",
    "print(pd.DataFrame(cm))\n",
    "\n",
    "report_rf = metrics.precision_recall_fscore_support(list(labels_test_1), list(pred), average='micro')\n",
    "print \"precision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\\n\" % \\\n",
    "        (report_rf[0], report_rf[1], report_rf[2], metrics.roc_auc_score(list(labels_test_1.values), list(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot roc curves for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
