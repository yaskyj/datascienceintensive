{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Flight Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Import, Manipulation, and Exploration\n",
    "### This initial section explores the data and cleans it for later modeling. This section also serves to complete the requirements for the Capstone Milestone report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import of basic libraries for data manipulation and plotting\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data consists of flight performance data from 2014 and the first half of 2015 with variables such as schedule departure date & time, airline carrier, origin airport, scheduled arrival time etc. with the dependent variable of ARR_DEL_15 (a flight delayed more than 15 minutes is considered delayed). This information was split into three separate CSV files. \n",
    "\n",
    "#### Weather data was also provided by location and in 15 minute increments, however, this set is not complete with respect to all information in the flight time performance data.\n",
    "\n",
    "#### The codes file was created manually by exporting flight city information from both the performance and weather data. Matches between the ICAO codes and IATA were either obvious or had to be researched online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training files are divided into three separate files in six month increments\n",
    "temp_train_2014_1 = pd.read_csv('CAX_Train_2014_Jan_to_Jun.csv')\n",
    "temp_train_2014_2 = pd.read_csv('CAX_Train_2014_Jul_to_Dec.csv')\n",
    "temp_train_2015 = pd.read_csv('CAX_Train_2015.csv')\n",
    "\n",
    "# Read in the weather file\n",
    "weather = pd.read_csv('weather.csv')\n",
    "\n",
    "# The codes file is a manual matching I created between the IATA codes in the \n",
    "# training files and the ICAO codes in the weather file. Multiple IATA codes can \n",
    "# join to the same ICAO codes based on proximity to cities. City names in the \n",
    "# training files can be truncated and often did not match the weather file. \n",
    "# If the matching wasn't obvious based on the IATA and ICAO codes then IATA \n",
    "# code were looked up in Google to determine thenearest ICAO city.\n",
    "codes = pd.read_csv('training_weather_cities_joined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine all of the training files into one dataframe\n",
    "training = pd.concat([temp_train_2014_1, temp_train_2014_2, temp_train_2015], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print training.dtypes\n",
    "print \"Number of columns in the flight data:\",len(training.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional time features are added in order to identify possible trends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create day, day of year, week of year, and hour features from the flight date\n",
    "training['DAY_OF_YEAR'] = pd.to_datetime(training['FL_DATE'], \n",
    "                                         errors='coerce').dt.dayofyear.astype(int)\n",
    "training['WEEK_OF_YEAR'] = pd.to_datetime(training['FL_DATE'], \n",
    "                                          errors='coerce').dt.weekofyear.astype(int)\n",
    "training['hour'] = training['CRS_DEP_TIME'].map(lambda x: int(str(int(x)).zfill(4)[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This array defines the dates of holidays in 2014 and 2015 which were looked up on the internet\n",
    "holidays = [\n",
    "        date(2014, 1, 1), date(2014, 1, 20), date(2014, 5, 26), date(2014, 7, 4),\n",
    "        date(2014, 9, 1), date(2014, 11, 27), date(2014, 12, 25),\n",
    "        date(2015, 1, 1), date(2015, 1, 19), date(2015, 5, 25), date(2015, 7, 4),\n",
    "        date(2015, 9, 7), date(2015, 11, 26), date(2015, 12, 25)\n",
    "     ]\n",
    "\n",
    "#Function that gives the number of days from a date to the nearest holiday\n",
    "def days_from_nearest_holiday(year, month, day):\n",
    "  d = date(year, month, day)\n",
    "  x = [(abs(d-h)).days for h in holidays]\n",
    "  return min(x)\n",
    "\n",
    "# Create the days from holiday feature using the defined function\n",
    "training['days_from_holidays'] = [days_from_nearest_holiday(r.YEAR, r.MONTH, r.day) \\\n",
    "                                  for i,r in training.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight Performance Data\n",
    "#### Below are various charts to discover possible variables impacting flight delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights per month\n",
    "grouped = training[['ARR_DEL15', 'MONTH']].groupby('MONTH').mean()\n",
    "\n",
    "#Sort months from highest to lowest based on mean flight delays\n",
    "grouped2 = grouped.sort(['ARR_DEL15'], ascending=False)\n",
    "\n",
    "# plot average delays by month\n",
    "grouped2.plot(kind='bar')\n",
    "plt.title('Mean Flight Delays by Month', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights per month\n",
    "grouped = training[['ARR_DEL15', 'DAY_OF_MONTH']].groupby('DAY_OF_MONTH').mean()\n",
    "\n",
    "#Sort days from highest to lowest based on mean flight delays\n",
    "grouped2 = grouped.sort(['ARR_DEL15'], ascending=False)\n",
    "\n",
    "# plot average delays by month\n",
    "grouped2.plot(kind='bar')\n",
    "plt.title('Mean Flight Delays by Day of Month', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights by hour\n",
    "grouped = training[['ARR_DEL15', 'hour']].groupby('hour').mean()\n",
    "\n",
    "# plot average delays by hour of day\n",
    "grouped.plot(kind='bar')\n",
    "plt.title('Mean Flight Delays by Hour of Day', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights per carrier and filter based on carriers\n",
    "# with more than 15 records\n",
    "grouped = training[['ARR_DEL15', 'UNIQUE_CARRIER']] \\\n",
    "    .groupby('UNIQUE_CARRIER').filter(lambda x: len(x)>15)\n",
    "grouped1 = grouped.groupby('UNIQUE_CARRIER').mean()\n",
    "\n",
    "# display all carriers by delay\n",
    "grouped1.sort(['ARR_DEL15'], ascending=False).plot(kind='bar')\n",
    "plt.title('Mean Flight Delays by Carrier', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights per origin airport based on airports\n",
    "# with more than 15 records\n",
    "grouped = training[['ARR_DEL15', 'ORIGIN']].groupby('ORIGIN').filter(lambda x: len(x)>15)\n",
    "grouped1 = grouped.groupby('ORIGIN').mean()\n",
    "\n",
    "# display top 50 origin airports by delay\n",
    "grouped1.sort(['ARR_DEL15'], ascending=False)[:50].plot(kind='bar')\n",
    "plt.title('Mean Flight Delays by Origin Airport', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute average number of delayed flights per destination airport\n",
    "grouped = training[['ARR_DEL15', 'DEST']].groupby('DEST').filter(lambda x: len(x)>15)\n",
    "grouped1 = grouped.groupby('DEST').mean()\n",
    "\n",
    "# display top 50 origin airports by delay\n",
    "grouped1.sort(['ARR_DEL15'], ascending=False)[:50].plot(kind='bar')\n",
    "plt.title('Mean Flight Delays by Destination Airport', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above charts, it appears that some of the features have a definite impact on flight delays. \n",
    "\n",
    "Flights delays by hour of day dramatically increase throughout the day peaking at around 8pm. Possibly this is due to problems building on each other during the course of the day.\n",
    "\n",
    "It's also obvious that certain carriers are more prone to delays. Origin airport appears to impact possible delays more than destination airports, but both charts start to even out quickly based on the top 50's trend.\n",
    "\n",
    "Finally, both month of year and day of month don't appear to have much variation when considering flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather features\n",
    "\n",
    "#### Weather data is cleaned and manipulated below. It is then grouped by week of year and joined to the flight performance dataframe for both origin and destination airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating week of year field in order to group weather events by week\n",
    "weather['week_of_year'] = pd.to_datetime(weather['date'], errors='coerce').dt.weekofyear.astype(\n",
    "    int)\n",
    "\n",
    "# Some basic munging of weather fields\n",
    "weather.events.fillna(-1, inplace=True)\n",
    "weather.conditions.fillna(-1, inplace=True)\n",
    "weather.gust_speed_mph.replace('-', -1, inplace=True)\n",
    "weather.gust_speed_mph.fillna(-1, inplace=True)\n",
    "weather.gust_speed_mph = pd.to_numeric(weather.gust_speed_mph, errors='coerce')\n",
    "weather.wind_speed_mph.replace('Calm', 0, inplace=True)\n",
    "weather.wind_speed_mph.fillna(-1, inplace=True)\n",
    "weather.wind_speed_mph = pd.to_numeric(weather.wind_speed_mph, errors='coerce')\n",
    "\n",
    "# Changing to weather types in the events field to numeric values based on the likelihood \n",
    "# of causing delays (e.g. Events involving ice have a higher values)\n",
    "weather.events.replace(['Fog', 'Fog-Hail-Thunderstorm', 'Fog-Rain', 'Fog-Rain-Hail-Thunderstorm', \n",
    "                        'Fog-Rain-Snow', 'Fog-Rain-Thunderstorm', \n",
    "                        'Fog-Rain-Thunderstorm-Tornado', 'Fog-Snow', 'Fog-Snow-Thunderstorm', \n",
    "                        'Fog-Thunderstorm', 'Hail', 'Hail-Thunderstorm', 'Rain', 'Rain-Hail', \n",
    "                        'Rain-Hail-Thunderstorm', 'Rain-Snow', 'Rain-Snow-Thunderstorm', \n",
    "                        'Rain-Thunderstorm', 'Rain-Thunderstorm-Tornado', 'Rain-Tornado', \n",
    "                        'Snow', 'Snow-Hail', 'Snow-Thunderstorm', 'Snow-Tornado', 'Thunderstorm', \n",
    "                        'Thunderstorm-Tornado', 'Tornado'], [10, 80, 20, 80, 60, 50, 100, 60, \n",
    "                         60, 50, 80, 80, 20, 80, 80, 60, 60, 50, 100, 100, 60, 80, 60, 100, \n",
    "                         50, 100, 100], inplace=True)\n",
    "\n",
    "# As with the events field, changing the conditions field to numeric values based on the \n",
    "# likelihood of causing delays\n",
    "weather.conditions.replace(['Unknown', 'Clear', 'Overcast', 'Partly Cloudy', 'Drizzle', \n",
    "                            'Scattered Clouds', 'Mostly Cloudy', 'Haze', 'Mist', \n",
    "                            'Patches of Fog', 'Rain', 'Shallow Fog', 'Low Drifting Snow', \n",
    "                            'Rain Showers', 'Light Blowing Snow', 'Light Drizzle', 'Light Fog', \n",
    "                            'Light Hail', 'Light Ice Pellets', 'Light Low Drifting Snow', \n",
    "                            'Light Mist', 'Light Rain', 'Light Rain Showers', 'Light Sand', \n",
    "                            'Light Small Hail Showers', 'Light Smoke', 'Light Snow', \n",
    "                            'Light Snow Grains', 'Light Snow Showers', 'Light Thunderstorm', \n",
    "                            'Light Thunderstorms and Rain', 'Light Thunderstorms and Snow', \n",
    "                            'Light Thunderstorms with Hail', \n",
    "                            'Light Thunderstorms with Small Hail', 'Heavy Blowing Snow', \n",
    "                            'Heavy Drizzle', 'Heavy Freezing Drizzle', 'Heavy Freezing Fog', \n",
    "                            'Heavy Freezing Rain', 'Heavy Ice Pellets', 'Heavy Rain', \n",
    "                            'Heavy Rain Showers', 'Heavy Small Hail', 'Heavy Snow', \n",
    "                            'Heavy Snow Showers', 'Heavy Thunderstorm', \n",
    "                            'Heavy Thunderstorms and Rain', 'Heavy Thunderstorms and Snow', \n",
    "                            'Heavy Thunderstorms with Hail', \n",
    "                            'Heavy Thunderstorms with Small Hail', 'Ice Crystals', \n",
    "                            'Ice Pellets', 'Funnel Cloud', 'Blowing Sand', 'Blowing Snow', \n",
    "                            'Thunderstorm', 'Thunderstorms and Rain', 'Thunderstorms and Snow', \n",
    "                            'Thunderstorms with Hail', 'Thunderstorms with Small Hail', \n",
    "                            'Volcanic Ash', 'Widespread Dust', 'Sand', 'Sandstorm', \n",
    "                            'Small Hail', 'Small Hail Showers', 'Smoke', 'Snow', 'Snow Grains', \n",
    "                            'Snow Showers', 'Squalls', 'Fog', 'Freezing Rain', 'Hail', \n",
    "                            'Light Freezing Drizzle', 'Light Freezing Fog', \n",
    "                            'Light Freezing Rain'], [-1, 0, 0, 0, 10, 0, 0, 0, 0, 10, 10, \n",
    "                             10, 30, 10, 30, 10, 10, 50, 70, 30, 0, 10, 10, 30, 50, 20, 30, \n",
    "                             70, 30, 20, 20, 40, 60, 60, 80, 20, 70, 70, 90, 80, 20, 20, 70, \n",
    "                             80, 80, 50, 30, 80, 80, 80, 70, 70, 90, 70, 70, 40, 30, 70, 80, \n",
    "                             80, 100, 20, 60, 80, 60, 70, 20, 60, 60, 60, 70, 10, 80, 50, 60, \n",
    "                             50, 80], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grouping the weather features by week of year and airport\n",
    "weather_group = weather.groupby( [ 'airport_code', 'week_of_year'] ).mean()\n",
    "weather_group = weather_group.reset_index()\n",
    "weather_group = weather_group.drop('zip', 1)\n",
    "weather_group.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the codes file to get the IACO codes for the origin IATA code\n",
    "training = pd.merge(training, codes[['ORIGIN', 'city', 'airport_code']], \n",
    "                    left_on='ORIGIN', right_on='ORIGIN')\n",
    "training.rename(columns={'city': 'origin_city', 'airport_code': 'origin_icao_code'}, inplace=True)\n",
    "\n",
    "# Merge the codes file to get the IACO codes for the destination IATA code\n",
    "training = pd.merge(training, codes[['ORIGIN', 'city', 'airport_code']], \n",
    "                    left_on='DEST', right_on='ORIGIN')\n",
    "training.rename(columns={'city': 'dest_city', 'airport_code': 'dest_icao_code', \n",
    "                         'ORIGIN_x': 'ORIGIN'}, inplace=True)\n",
    "training = training.drop('ORIGIN_y', 1)\n",
    "\n",
    "# Merge the weather data with the training data on the origin and week of year\n",
    "training = pd.merge(training, weather_group, left_on=['origin_icao_code', 'WEEK_OF_YEAR'], \n",
    "                    right_on=['airport_code', 'week_of_year'], how='left')\n",
    "training.rename(columns={'conditions': 'orig_conditions', 'airport_code': 'orig_airport_code', \n",
    "                         'week_of_year': 'orig_week_of_year', \n",
    "                         'temperature_f': 'orig_temperature_f', \n",
    "                         'dew_point_f': 'orig_dew_point_f', 'humidity': 'orig_humidity', \n",
    "                         'sea_level_pressure_in': 'orig_sea_level_pressure_in', \n",
    "                         'visibility_mph': 'orig_visibility_mph', \n",
    "                         'wind_speed_mph': 'orig_wind_speed_mph', \n",
    "                         'gust_speed_mph': 'orig_gust_speed_mph', \n",
    "                         'precipitation_in': 'orig_precipitation_in', \n",
    "                         'events': 'orig_events', 'wind_dir_degrees': 'orig_wind_dir_degrees'\\\n",
    "                        }, inplace=True)\n",
    "\n",
    "# Merge the weather data with the training data on the destination and week of year\n",
    "training = pd.merge(training, weather_group, left_on=['dest_icao_code', 'WEEK_OF_YEAR'], \n",
    "                    right_on=['airport_code', 'week_of_year'], how='left')\n",
    "training.rename(columns={'conditions': 'dest_conditions', 'airport_code': 'dest_airport_code', \n",
    "                         'week_of_year': 'dest_week_of_year', \n",
    "                         'temperature_f': 'dest_temperature_f', \n",
    "                         'dew_point_f': 'dest_dew_point_f', 'humidity': 'dest_humidity', \n",
    "                         'sea_level_pressure_in': 'dest_sea_level_pressure_in', \n",
    "                         'visibility_mph': 'dest_visibility_mph', \n",
    "                         'wind_speed_mph': 'dest_wind_speed_mph', \n",
    "                         'gust_speed_mph': 'dest_gust_speed_mph', \n",
    "                         'precipitation_in': 'dest_precipitation_in', \n",
    "                         'events': 'dest_events', 'wind_dir_degrees': 'dest_wind_dir_degrees'\\\n",
    "                        }, inplace=True)\n",
    "\n",
    "training.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are some charts exploring the impact of some weather features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grouping wind degrees into bins and getting the mean delay\n",
    "grouped = training[['ARR_DEL15']].groupby(pd.cut(training.orig_wind_dir_degrees, \\\n",
    "                                     np.arange(0, 350, 50))).mean()\n",
    "\n",
    "grouped.plot(kind='bar')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Mean Flight Delays by Origin Wind Degrees', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grouping inches of precipitation into bins and getting the mean delay\n",
    "grouped = training[['ARR_DEL15']].groupby(pd.cut(training.orig_precipitation_in, \\\n",
    "                                     np.arange(0, 1.1, .1))).mean()\n",
    "\n",
    "grouped.plot(kind='bar')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Mean Flight Delays by Origin Precipitation in Inches', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grouping gust speed into bins and getting the mean delay\n",
    "grouped = training[['ARR_DEL15']].groupby(pd.cut(training.orig_gust_speed_mph, \\\n",
    "                                     np.arange(0, 30, 5))).mean()\n",
    "\n",
    "grouped.plot(kind='bar')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Mean Flight Delays by Origin Gust Speed in MPH', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = training[['ARR_DEL15']].groupby(pd.cut(training.orig_temperature_f, \\\n",
    "                                     np.arange(-25, 110, 10))).mean()\n",
    "\n",
    "grouped.plot(kind='bar')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Mean Flight Delays by Origin Temperatures', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Grouping wind speed into bins and getting the mean delay\n",
    "grouped = training[['ARR_DEL15']].groupby(pd.cut(training.orig_wind_speed_mph, \\\n",
    "                                     np.arange(0, 30, 5))).mean()\n",
    "\n",
    "grouped.plot(kind='bar')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Mean Flight Delays by Origin Wind Speeds in MPH', fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that some weather events have an impact on flight delays.\n",
    "\n",
    "The origin airports gust wind speeds appear to impact flights once it gets above 20 MPHs. Precipitation levels of around .7-.8 inches show increased flight delays. Temperatures between 5-25 degrees show increased impact on delays.\n",
    "\n",
    "Both origin airport wind speeds and wind degrees do not appear to make a significant difference in flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the data has been explored, it needs to be reduced in order to build models.\n",
    "\n",
    "The training data will be exported into a CSV file. The iPython notebook will be restarted to release memory and then the smaller training set will be read back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There's a large difference in the amount of data for delayed vs. on time flights\n",
    "print \"Number of delayed flights:\",len(training[training['ARR_DEL15'] == 1])\n",
    "print \"Number of on-time flights:\",len(training[training['ARR_DEL15'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define the features in order to export the training set to a csv for quicker reinput\n",
    "features_for_export = [c for c in training.columns if c in \\\n",
    "                       ['ARR_DEL15', 'days_from_holidays', 'DAY_OF_MONTH', 'DAY_OF_WEEK', \n",
    "                        'UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'DISTANCE', 'hour', \n",
    "                        'MONTH', 'orig_temperature_f', 'orig_visibility_mph', \n",
    "                        'orig_wind_speed_mph', 'orig_gust_speed_mph', \n",
    "                        'orig_precipitation_in', 'orig_wind_dir_degrees', \n",
    "                        'dest_temperature_f','dest_visibility_mph', \n",
    "                        'dest_wind_speed_mph', 'dest_gust_speed_mph', \n",
    "                        'dest_precipitation_in', 'dest_wind_dir_degrees']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reduce the number of on time training instances so that the model will fit more easily into \n",
    "# memory.\n",
    "# Keep all delayed instances and take random instances of on time flights equal to the number of\n",
    "# delayed instances\n",
    "delayed = training[training['ARR_DEL15'] == 1]\n",
    "on_time = training[training['ARR_DEL15'] == 0]\n",
    "sample_rows = np.random.choice(on_time.index.values, len(delayed))\n",
    "on_time = on_time.ix[sample_rows]\n",
    "training = pd.concat([on_time, delayed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training[features_for_export].to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Flight Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation, ensemble, tree, metrics, preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, date\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "le = preprocessing.LabelEncoder()\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the reduced training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create training set from combined data already saved to csv\n",
    "training = pd.read_csv('combined_data.csv', iterator=True, chunksize=100000)\n",
    "training = pd.concat([chunk for chunk in training], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define the features to use\n",
    "features = [c for c in training.columns if c in ['days_from_holidays', 'DAY_OF_MONTH',\n",
    "                                                 'DAY_OF_WEEK', 'UNIQUE_CARRIER', 'ORIGIN', \n",
    "                                                 'DEST', 'DISTANCE', 'hour', 'MONTH', \n",
    "                                                 'orig_temperature_f', 'orig_visibility_mph', \n",
    "                                                 'orig_wind_speed_mph', 'orig_gust_speed_mph', \n",
    "                                                 'orig_precipitation_in', 'orig_wind_dir_degrees', \n",
    "                                                 'dest_temperature_f', 'dest_visibility_mph', \n",
    "                                                 'dest_wind_speed_mph', 'dest_gust_speed_mph', \n",
    "                                                 'dest_precipitation_in', 'dest_wind_dir_degrees']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = training['ARR_DEL15']\n",
    "training = training.drop('ARR_DEL15', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the string labels to numerical using label encoder \n",
    "training.UNIQUE_CARRIER = le.fit_transform(training.UNIQUE_CARRIER)\n",
    "training.ORIGIN = le.fit_transform(training.ORIGIN)\n",
    "training.DEST = le.fit_transform(training.DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categ = [list(training.columns).index(x) for x in 'DAY_OF_MONTH', 'DAY_OF_WEEK', \n",
    "         'UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'hour']\n",
    "enc = preprocessing.OneHotEncoder(categorical_features = categ)\n",
    "training['UNIQUE_CARRIER'] = pd.factorize(training['UNIQUE_CARRIER'])[0]\n",
    "training['ORIGIN'] = pd.factorize(training['ORIGIN'])[0]\n",
    "training['DEST'] = pd.factorize(training['DEST'])[0]\n",
    "sparse_train = enc.fit_transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create training and testing sets using cross validation\n",
    "features_train_1, features_test_1, labels_train_1, labels_test_1 = \\\n",
    "    cross_validation.train_test_split(sparse_train, labels, test_size=0.70)\n",
    "features_train_2, features_test_2, labels_train_2, labels_test_2 = \\\n",
    "    cross_validation.train_test_split(features_train_1, labels_train_1, test_size=0.70)\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    cross_validation.train_test_split(features_train_2, labels_train_2, test_size=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# naive = GaussianNB().fit(features_train, labels_train)\n",
    "forest = ensemble.RandomForestClassifier(n_estimators=10, \n",
    "                                         n_jobs=-1).fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "pred = forest.predict(features_test_1)\n",
    "\n",
    "# print results\n",
    "cm = metrics.confusion_matrix(labels_test_1, pred)\n",
    "print(\"Confusion matrix\")\n",
    "print(pd.DataFrame(cm))\n",
    "\n",
    "report_rf = metrics.precision_recall_fscore_support(list(labels_test_1), \n",
    "                                                    list(pred), average='micro')\n",
    "print \"Precision = %0.2f, Recall = %0.2f, F1 = %0.2f, Area Under Curve = %0.2f\\n\" % \\\n",
    "        (report_rf[0], report_rf[1], report_rf[2], \n",
    "         metrics.roc_auc_score(list(labels_test_1.values), list(pred)))\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(list(labels_test_1), list(pred))\n",
    "roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, features, labels, useTrainCV=True, cv_folds=5, \n",
    "             early_stopping_rounds=20):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(features, label=labels)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], \n",
    "                          nfold=cv_folds,\n",
    "            metrics=['auc'], early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(features, labels, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(features)\n",
    "    dtrain_predprob = alg.predict_proba(features)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print \"\\nModel Report\"\n",
    "    print \"Accuracy : %.4g\" % metrics.accuracy_score(labels.values, dtrain_predictions)\n",
    "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(labels, dtrain_predprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boost =  xgb.XGBClassifier(\n",
    " learning_rate = 1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "modelfit(boost, features_train_1, labels_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "boost =  xgb.XGBClassifier(\n",
    " learning_rate = 1,\n",
    " n_estimators=329,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27).fit(features_train_1, labels_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate on test set\n",
    "pred = boost.predict(features_test_1)\n",
    "\n",
    "\n",
    "# print results\n",
    "cm = metrics.confusion_matrix(labels_test_1, pred)\n",
    "print(\"Confusion matrix\")\n",
    "print(pd.DataFrame(cm))\n",
    "\n",
    "report_rf = metrics.precision_recall_fscore_support(list(labels_test_1), list(pred), \n",
    "                                                    average='micro')\n",
    "print \"precision = %0.2f, recall = %0.2f, F1 = %0.2f, accuracy = %0.2f\\n\" % \\\n",
    "        (report_rf[0], \n",
    "         report_rf[1], \n",
    "         report_rf[2], \n",
    "         metrics.roc_auc_score(list(labels_test_1.values), list(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot roc curves for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
